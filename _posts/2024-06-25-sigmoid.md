---
layout: post
title: "Sigmoid: the 'natural' choice"
---

One of the very first models studied in any introductory machine learning course is the logistic regression model for binary classification:

$$\hat y_i = \sigma(X_i \beta) \tag{1.0}$$

Here, $X_i = [\: x_{i1} \: x_{i2} \: \dots \: x_{id}\:]$ are the features (also called covariates) of an input we want to classify. For example, these could be the pixels of an image, or a vector representation of some text. We'll also assume $x_{i1} = 1$, so as to include a bias term in our model. $\beta$ is a $d$-dimensional column vector of our learnable model parameters, and $\hat y_i$ is the model output. The function $\sigma$ is the **sigmoid function**, and it is given by: 

$$\sigma(z) = \frac{1}{1 + \exp(-z)} \tag{1.1}$$

In an introductory ML course, the sigmoid function is produced seemingly out of nowhere, and it happens to be exactly what we need to make our model work:

- The range of $\sigma$ is $(0, 1)$, which means our model output $\hat y_i$ can be interpreted as a probability distribution (more on this later). Hence our model can make classification predictions simply by rounding $\hat y_i$ to the nearest integer, either 0 or 1.
- $\sigma$ is differentiable everywhere, and its derivative can be elegantly expressed as $\sigma'(z) = \sigma(z)(1 - \sigma(z))$. In particular the derivative is nowhere 0, because $\sigma(z)$ is within $(0,1)$ for any $z$. This is a useful property if we are training our model using gradient descent.
\end{itemize}

But certainly sigmoid isn't the only differentiable function to possess these desirable properties. What makes sigmoid so well-suited for binary classification? Where does the expression for sigmoid (1.1) come from? In the following sections I will present an answer to these questions from a statistical perspective, and explain why sigmoid is in fact the most “natural” choice for our binary classification model. 